"""
æ–‡æ¡£å¤„ç† API è·¯ç”±
æä¾› PDFã€Wordã€CADã€BIM æ–‡ä»¶çš„ä¸Šä¼ å’Œè§£æåŠŸèƒ½
"""
import os
import json
import aiofiles
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from ..core.config import get_settings
from ..utils.pdf_parser import parse_pdf_file, PDFContent
from ..services.graphiti_service import get_graphiti_service, KnowledgeGraphResult

logger = logging.getLogger(__name__)
settings = get_settings()

router = APIRouter()

# æ–‡æ¡£æ•°æ®å­˜å‚¨æ–‡ä»¶è·¯å¾„
DOCUMENTS_DB_PATH = os.path.join(settings.UPLOAD_DIR, "documents_db.json")

# å“åº”æ¨¡å‹
class FileUploadResponse(BaseModel):
    """æ–‡ä»¶ä¸Šä¼ å“åº”"""
    success: bool
    file_id: str
    filename: str
    file_size: int
    file_type: str
    message: str


class DocumentParseResponse(BaseModel):
    """æ–‡æ¡£è§£æå“åº”"""
    success: bool
    file_id: str
    content: PDFContent
    processing_time: float
    message: str


class ProcessingStatus(BaseModel):
    """å¤„ç†çŠ¶æ€"""
    file_id: str
    status: str  # uploading, parsing, building_graph, completed, error
    progress: int  # 0-100
    message: str
    result: Dict[str, Any] = {}


class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯"""
    file_id: str
    filename: str
    file_type: str
    file_size: int
    upload_time: datetime
    status: str = "uploaded"
    processed_at: Optional[datetime] = None
    node_count: int = 0
    error_message: Optional[str] = None


def load_documents_db() -> Dict[str, DocumentInfo]:
    """åŠ è½½æ–‡æ¡£æ•°æ®åº“"""
    try:
        if os.path.exists(DOCUMENTS_DB_PATH):
            with open(DOCUMENTS_DB_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # è½¬æ¢ä¸ºDocumentInfoå¯¹è±¡
                documents = {}
                for file_id, doc_data in data.items():
                    # å¤„ç†datetimeå­—æ®µ
                    if doc_data.get('upload_time') and isinstance(doc_data['upload_time'], str):
                        doc_data['upload_time'] = datetime.fromisoformat(doc_data['upload_time'].replace('Z', '+00:00'))
                    if doc_data.get('processed_at') and isinstance(doc_data['processed_at'], str):
                        doc_data['processed_at'] = datetime.fromisoformat(doc_data['processed_at'].replace('Z', '+00:00'))
                    
                    documents[file_id] = DocumentInfo(**doc_data)
                return documents
        return {}
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡æ¡£æ•°æ®åº“å¤±è´¥: {e}")
        return {}


def save_documents_db(documents: Dict[str, DocumentInfo]):
    """ä¿å­˜æ–‡æ¡£æ•°æ®åº“"""
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(DOCUMENTS_DB_PATH), exist_ok=True)
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        data = {}
        for file_id, doc_info in documents.items():
            doc_dict = doc_info.model_dump()
            # è½¬æ¢datetimeä¸ºISOå­—ç¬¦ä¸²
            if doc_dict.get('upload_time'):
                doc_dict['upload_time'] = doc_dict['upload_time'].isoformat()
            if doc_dict.get('processed_at'):
                doc_dict['processed_at'] = doc_dict['processed_at'].isoformat()
            data[file_id] = doc_dict
        
        with open(DOCUMENTS_DB_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡æ¡£æ•°æ®åº“å¤±è´¥: {e}")


def get_document_info(file_id: str) -> DocumentInfo:
    """è·å–æ–‡æ¡£ä¿¡æ¯"""
    documents = load_documents_db()
    return documents.get(file_id)


def update_document_status(file_id: str, status: str, **kwargs):
    """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
    documents = load_documents_db()
    if file_id in documents:
        documents[file_id].status = status
        for key, value in kwargs.items():
            if hasattr(documents[file_id], key):
                setattr(documents[file_id], key, value)
        save_documents_db(documents)


# å…¨å±€çŠ¶æ€å­˜å‚¨ (ç®€åŒ–å®ç°ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis æˆ–æ•°æ®åº“)
processing_status: Dict[str, ProcessingStatus] = {}


def validate_file(file: UploadFile) -> None:
    """éªŒè¯ä¸Šä¼ çš„æ–‡ä»¶"""
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    if file.size > settings.MAX_FILE_SIZE:
        raise HTTPException(
            status_code=413,
            detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({settings.MAX_FILE_SIZE / 1024 / 1024:.1f}MB)"
        )
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in settings.ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(settings.ALLOWED_EXTENSIONS)}"
        )


def get_file_type(filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶åè·å–æ–‡ä»¶ç±»å‹"""
    ext = Path(filename).suffix.lower()
    
    type_map = {
        '.pdf': 'pdf',
        '.doc': 'doc',
        '.docx': 'doc', 
        '.dxf': 'cad',
        '.dwg': 'cad',
        '.ifc': 'bim'
    }
    
    return type_map.get(ext, 'unknown')


@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """
    ä¸Šä¼ æ–‡ä»¶
    
    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š
    - PDF: .pdf
    - Word: .doc, .docx
    - CAD: .dxf, .dwg
    - BIM: .ifc
    """
    try:
        # éªŒè¯æ–‡ä»¶
        validate_file(file)
        
        # ç”Ÿæˆæ–‡ä»¶ID
        import uuid
        file_id = str(uuid.uuid4())
        
        # ç¡®å®šæ–‡ä»¶ç±»å‹å’Œå­˜å‚¨è·¯å¾„
        file_type = get_file_type(file.filename)
        storage_dir = os.path.join(settings.UPLOAD_DIR, file_type)
        os.makedirs(storage_dir, exist_ok=True)
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_filename = f"{file_id}_{file.filename}"
        file_path = os.path.join(storage_dir, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶
        async with aiofiles.open(file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
        
        # åˆ›å»ºæ–‡æ¡£ä¿¡æ¯å¹¶ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        doc_info = DocumentInfo(
            file_id=file_id,
            filename=file.filename,
            file_type=file_type,
            file_size=len(content),
            upload_time=datetime.now(),
            status="uploaded"
        )
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–æ•°æ®åº“
        documents = load_documents_db()
        documents[file_id] = doc_info
        save_documents_db(documents)
        
        # è®°å½•å¤„ç†çŠ¶æ€
        processing_status[file_id] = ProcessingStatus(
            file_id=file_id,
            status="uploaded",
            progress=100,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ"
        )
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename} -> {file_path}")
        
        return FileUploadResponse(
            success=True,
            file_id=file_id,
            filename=file.filename,
            file_size=len(content),
            file_type=file_type,
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ"
        )
        
    except HTTPException as http_exc:
        logger.warning(f"HTTPException during file upload for '{file.filename}': {http_exc.detail}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error during file upload for '{file.filename}'", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚")


def find_file_by_id(file_id: str) -> tuple[str, str] | None:
    """æ ¹æ®æ–‡ä»¶IDæŸ¥æ‰¾æ–‡ä»¶è·¯å¾„å’ŒåŸå§‹æ–‡ä»¶å"""
    for subdir in ["pdf", "doc", "cad", "bim"]:
        search_dir = os.path.join(settings.UPLOAD_DIR, subdir)
        if os.path.exists(search_dir):
            for f in os.listdir(search_dir):
                if f.startswith(file_id + "_"):
                    file_path = os.path.join(search_dir, f)
                    filename = f[len(file_id) + 1:]  # ç§»é™¤ file_id å‰ç¼€
                    return file_path, filename
    return None


@router.get("/status/{file_id}", response_model=ProcessingStatus)
async def get_processing_status(file_id: str):
    """è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€"""
    try:
        documents = load_documents_db()
        if file_id not in documents:
            logger.warning(f"Status requested for non-existent file_id: {file_id}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶IDä¸å­˜åœ¨")

        doc_info = documents[file_id]

        # æ ¹æ®æ–‡æ¡£çŠ¶æ€è®¡ç®—è¿›åº¦å’Œæ¶ˆæ¯
    progress_map = {
        "uploaded": 0,
        "processing": 50,
        "completed": 100,
        "failed": 100
    }
    
    message_map = {
        "uploaded": "æ–‡ä»¶å·²ä¸Šä¼ ï¼Œç­‰å¾…å¤„ç†",
        "processing": "æ­£åœ¨å¤„ç†æ–‡æ¡£...",
        "completed": "æ–‡æ¡£å¤„ç†å®Œæˆ",
        "failed": f"å¤„ç†å¤±è´¥: {doc_info.error_message or 'æœªçŸ¥é”™è¯¯'}"
    }
    
    return ProcessingStatus(
        file_id=file_id,
        status=doc_info.status,
        progress=progress_map.get(doc_info.status, 0),
        message=message_map.get(doc_info.status, "æœªçŸ¥çŠ¶æ€"),
        result={
            "node_count": doc_info.node_count,
            "processed_at": doc_info.processed_at.isoformat() if doc_info.processed_at else None,
            "error_message": doc_info.error_message
        }
    )
    except HTTPException as http_exc:
        # This will be caught by the global exception handler if not handled here,
        # but explicit logging can be useful.
        logger.warning(f"HTTPException in get_processing_status for file_id '{file_id}': {http_exc.detail}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error in get_processing_status for file_id '{file_id}'", exc_info=True)
        raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè·å–çŠ¶æ€å¤±è´¥ã€‚")


async def _parse_document_internal(file_id: str, enable_ocr: bool = True):
    """å†…éƒ¨è§£æå‡½æ•°ï¼Œè¿”å›åŸå§‹æ•°æ®è€Œä¸æ˜¯JSONResponse"""
    # This is an internal helper, so direct exception handling might be less critical
    # if the calling function handles them. However, adding some for robustness.
    try:
        # æ£€æŸ¥æ–‡ä»¶IDï¼Œå¦‚æœå†…å­˜ä¸­æ²¡æœ‰åˆ™å°è¯•æ¢å¤
        if file_id not in processing_status:
            file_info_tuple = find_file_by_id(file_id) # Renamed to avoid conflict
            if file_info_tuple is None:
                logger.error(f"File ID {file_id} not found by find_file_by_id in _parse_document_internal.")
                raise HTTPException(status_code=404, detail="æ–‡ä»¶IDä¸å­˜åœ¨ (internal find failed)")
            
            # é‡å»ºå¤„ç†çŠ¶æ€
            file_path, filename = file_info_tuple
            processing_status[file_id] = ProcessingStatus(
                file_id=file_id,
                status="uploaded", # Or "unknown_recovered"
                progress=0, # Reset progress
                message="æ–‡ä»¶çŠ¶æ€å·²æ¢å¤ï¼ˆå¯èƒ½åœ¨æœåŠ¡é‡å¯åï¼‰"
            )
        
        # æ›´æ–°çŠ¶æ€
        processing_status[file_id].status = "parsing"
        processing_status[file_id].progress = 10
        processing_status[file_id].message = "æ­£åœ¨è§£ææ–‡æ¡£..."
        
        # æŸ¥æ‰¾æ–‡ä»¶
        file_info_tuple_lookup = find_file_by_id(file_id) # Renamed
        if file_info_tuple_lookup is None:
            processing_status[file_id].status = "error"
            processing_status[file_id].message = "æ–‡ä»¶ç‰©ç†è·¯å¾„æœªæ‰¾åˆ°"
            logger.error(f"Physical file for ID {file_id} not found during parsing attempt.")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ç‰©ç†è·¯å¾„æœªæ‰¾åˆ°")
        
        file_path, filename = file_info_tuple_lookup
        
        # æ›´æ–°è¿›åº¦
        processing_status[file_id].progress = 30
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è§£æ
        file_type = get_file_type(filename)
        
        if file_type == "pdf":
            from ..utils.pdf_parser import parse_pdf_file
            
            # è§£æPDFæ–‡ä»¶
            pdf_content = parse_pdf_file(file_path, enable_ocr=enable_ocr)
            
            # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼
            def ultra_clean_string(s):
                if not isinstance(s, str):
                    return str(s) if s is not None else ""

                cleaned = ""
                for char in s:
                    code = ord(char)
                    # Allow basic printable ASCII, common whitespace, and CJK Unified Ideographs
                    if (32 <= code <= 126) or char in '\n\r\t' or \
                       (0x4e00 <= code <= 0x9fff) or \
                       (0x3000 <= code <= 0x303f) or \
                       (0xff00 <= code <= 0xffef): # Added CJK Symbols/Punctuation and Fullwidth forms
                        cleaned += char
                    elif code > 127: # Keep other Unicode characters too for now
                        cleaned += char
                return cleaned.strip()

            def deep_clean_data(obj):
                if isinstance(obj, dict):
                    return {ultra_clean_string(str(k)): deep_clean_data(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [deep_clean_data(item) for item in obj]
                elif isinstance(obj, str):
                    return ultra_clean_string(obj)
                else:
                    return obj # Keep non-string, non-dict, non-list as is

            content_dict = {
                "text": ultra_clean_string(pdf_content.text),
                "page_count": pdf_content.page_count,
                "metadata": deep_clean_data(pdf_content.metadata),
                "images": deep_clean_data(pdf_content.images), # Images list might contain complex objects
                "has_forms": pdf_content.has_forms,
                "tables": deep_clean_data(pdf_content.tables) # Tables list
            }
            
            # æ›´æ–°çŠ¶æ€
            # This status update should ideally be in the calling function after success
            # processing_status[file_id].status = "completed"
            # processing_status[file_id].progress = 100
            # processing_status[file_id].message = "æ–‡æ¡£è§£æå®Œæˆ"
            # processing_status[file_id].result = {
            #     "content": content_dict,
            #     "processing_time": 0.0
            # }

            return {
                "success": True,
                "file_id": file_id,
                "content": content_dict, # This is the PDFContent model data, not yet JSON
                "message": "PDF è§£æå®Œæˆ"
            }
            
        else:
            logger.warning(f"Unsupported file type '{file_type}' for parsing in _parse_document_internal for file_id '{file_id}'.")
            raise HTTPException(
                status_code=400, 
                detail=f"æš‚ä¸æ”¯æŒ {file_type} æ–‡ä»¶ç±»å‹çš„è§£æ"
            )
    except HTTPException as http_exc:
        logger.error(f"HTTPException in _parse_document_internal for {file_id}: {http_exc.detail}", exc_info=True)
        if file_id in processing_status:
            processing_status[file_id].status = "error"
            processing_status[file_id].message = f"å†…éƒ¨è§£æé”™è¯¯: {http_exc.detail}"
        raise
    except Exception as e:
        logger.error(f"Unexpected error in _parse_document_internal for {file_id}", exc_info=True)
        if file_id in processing_status:
            processing_status[file_id].status = "error"
            processing_status[file_id].message = f"å†…éƒ¨è§£ææ„å¤–å¤±è´¥: {str(e)}"
        # Re-raise as a generic 500 if it's not an HTTPException already
        raise HTTPException(status_code=500, detail=f"å†…éƒ¨è§£ææ„å¤–å¤±è´¥: {str(e)}")


@router.post("/parse/{file_id}")
async def parse_document(file_id: str, enable_ocr: bool = True):
    """
    è§£ææ–‡æ¡£å†…å®¹

    ç›®å‰æ”¯æŒ PDF æ–‡æ¡£è§£æï¼Œåç»­ä¼šæ‰©å±•æ”¯æŒå…¶ä»–æ ¼å¼
    """
    start_time = datetime.now()
    try:
        # Initial status check and setup (simplified from original as _parse_document_internal handles some of this)
        doc_info_db = get_document_info(file_id)
        if not doc_info_db:
            logger.warning(f"Parse requested for non-existent file_id in DB: {file_id}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶IDåœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨")

        if file_id not in processing_status:
             processing_status[file_id] = ProcessingStatus(
                file_id=file_id, status="pending_parse", progress=0, message="è§£æä»»åŠ¡å·²åˆ›å»º"
            )

        processing_status[file_id].status = "parsing"
        processing_status[file_id].progress = 10
        processing_status[file_id].message = "æ­£åœ¨è§£ææ–‡æ¡£..."

        # Call internal parsing logic
        parsed_data = await _parse_document_internal(file_id, enable_ocr)

        processing_time_seconds = (datetime.now() - start_time).total_seconds()
        
        # Update status upon successful parsing
        processing_status[file_id].status = "parsed_successfully" # More specific status
        processing_status[file_id].progress = 100
        processing_status[file_id].message = "æ–‡æ¡£è§£ææˆåŠŸå®Œæˆ"
        # parsed_data["content"] is already a dict from _parse_document_internal
        processing_status[file_id].result = {
            "content_summary": {
                "page_count": parsed_data["content"]["page_count"],
                "text_length": len(parsed_data["content"]["text"]),
                "num_images": len(parsed_data["content"]["images"]),
                "num_tables": len(parsed_data["content"]["tables"]),
            },
            "processing_time": processing_time_seconds
        }

        # Update DB status
        update_document_status(file_id, "parsed_successfully", processed_at=datetime.now())

        # Return the actual parsed content along with success metrics
        # The PDFContent model fields might not be directly serializable by Pydantic if they contain complex types
        # _parse_document_internal now returns a dict, so this should be fine.
        final_response_content = {
            "success": True,
            "file_id": file_id,
            "content": parsed_data["content"], # This is the cleaned dict
            "processing_time": processing_time_seconds,
            "message": "æ–‡æ¡£è§£ææˆåŠŸå®Œæˆ"
        }
        return JSONResponse(content=final_response_content) # Ensure it's JSON serializable

    except HTTPException as http_exc:
        logger.warning(f"HTTPException during parsing for file_id '{file_id}': {http_exc.detail}")
        if file_id in processing_status:
            processing_status[file_id].status = "error_parsing"
            processing_status[file_id].message = f"æ–‡æ¡£è§£æå¤±è´¥: {http_exc.detail}"
        update_document_status(file_id, "error_parsing", error_message=http_exc.detail)
        raise
    except Exception as e:
        logger.error(f"Unexpected error during parsing for file_id '{file_id}'", exc_info=True)
        if file_id in processing_status:
            processing_status[file_id].status = "error_parsing"
            processing_status[file_id].message = f"æ–‡æ¡£è§£ææ„å¤–å¤±è´¥: {str(e)}"
        update_document_status(file_id, "error_parsing", error_message=str(e))
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œæ–‡æ¡£è§£æå¤±è´¥ã€‚")


async def build_knowledge_graph_background(file_id: str, pdf_content_dict: Dict[str, Any], filename: str):
    """åå°ä»»åŠ¡ï¼šæ„å»ºçŸ¥è¯†å›¾è°±. Takes dict instead of PDFContent model."""
    try:
        # Ensure status exists for file_id
        if file_id not in processing_status:
            processing_status[file_id] = ProcessingStatus(
                file_id=file_id, status="unknown", progress=0, message="BG task started for unknown file"
            )

        processing_status[file_id].status = "building_graph"
        processing_status[file_id].progress = 50 # Arbitrary progress for KG build start
        processing_status[file_id].message = "æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±..."
        update_document_status(file_id, "building_graph")
        
        graphiti_service = get_graphiti_service()
        if not graphiti_service or not graphiti_service.is_available():
            logger.error(f"Graphiti service not available for KG build for file_id {file_id}.")
            raise Exception("GraphitiæœåŠ¡ä¸å¯ç”¨æˆ–æœªåˆå§‹åŒ–")

        # Reconstruct PDFContent from dict for graphiti_service if it expects the model
        # For now, graphiti_service.build_knowledge_graph_from_pdf expects PDFContent model
        # but build_knowledge_graph expects text. We need to align this.
        # The current `build_knowledge_graph_from_pdf` in graphiti_service.py takes text chunks.
        # Let's assume for now the background task gets the text directly.
        # The `process_pdf_document` calls `graphiti_service.build_knowledge_graph` with text.
        # This `build_knowledge_graph_background` seems to be part of an older flow or needs text.
        # Re-aligning: this function should receive text, not a PDFContent object/dict.
        # However, the caller `process_document_background` -> `process_pdf_document` calls
        # `graphiti_service.build_knowledge_graph(text=pdf_content.text, ...)`
        # So this `build_knowledge_graph_background` might be unused or needs refactoring.
        # For now, let's assume it gets the text it needs from `pdf_content_dict['text']`.

        text_to_process = pdf_content_dict.get("text")
        if not text_to_process:
            logger.error(f"No text found in pdf_content_dict for KG build of {file_id}")
            raise ValueError("Text content is missing for knowledge graph construction.")

        # This call is problematic as build_knowledge_graph_from_pdf expects PDFContent model
        # and build_knowledge_graph expects just text.
        # Let's use the direct text method:
        result_dict = await graphiti_service.build_knowledge_graph(
            text=text_to_process, # Pass the text
            document_id=file_id # document_id is used as episode_name prefix
        )
        
        # Convert result (which is a dict) to KnowledgeGraphResult if needed, or handle dict directly
        # The current result_dict is already a dict with success, counts, etc.
        
        current_time = datetime.now()
        if result_dict.get("success"):
            processing_status[file_id].status = "completed"
            processing_status[file_id].message = "çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ"
            node_count = result_dict.get("actual_created_entities", 0) # Or total_graph_nodes
            update_document_status(file_id, "completed", processed_at=current_time, node_count=node_count, error_message=None)
            logger.info(f"âœ… æ–‡æ¡£ {filename} (ID: {file_id}) çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {node_count}")
        else:
            error_msg = result_dict.get("error", "çŸ¥è¯†å›¾è°±æ„å»ºæœªçŸ¥é”™è¯¯")
            processing_status[file_id].status = "error_kg_build"
            processing_status[file_id].message = f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {error_msg}"
            update_document_status(file_id, "error_kg_build", processed_at=current_time, error_message=error_msg)
            logger.error(f"âŒ æ–‡æ¡£ {filename} (ID: {file_id}) çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {error_msg}")

        processing_status[file_id].progress = 100
        # processing_status[file_id].result.update({"knowledge_graph": result_dict}) # Store full result if needed
        
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±åå°æ„å»ºå¤±è´¥ for file_id {file_id}: {str(e)}", exc_info=True)
        if file_id in processing_status: # Check again in case it was cleared
            processing_status[file_id].status = "error_kg_build"
            processing_status[file_id].message = f"çŸ¥è¯†å›¾è°±æ„å»ºå¼‚å¸¸: {str(e)}"
        update_document_status(file_id, "error_kg_build", processed_at=datetime.now(), error_message=str(e))


@router.post("/process/{file_id}")
async def process_document(
    file_id: str,
    background_tasks: BackgroundTasks
):
    """å¤„ç†å•ä¸ªæ–‡æ¡£"""
    try:
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        docs_db = load_documents_db()
        if file_id not in docs_db:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        doc_info = docs_db[file_id]
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å¤„ç†ä¸­
        if doc_info.status == "processing":
            return {"message": "æ–‡æ¡£æ­£åœ¨å¤„ç†ä¸­", "status": "processing"}
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        doc_info.status = "processing"
        doc_info.processed_at = datetime.now()
        docs_db[file_id] = doc_info
        save_documents_db(docs_db)
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_info.filename} (ID: {file_id})")
        
        # å¯åŠ¨åå°å¤„ç†ä»»åŠ¡ - ä¼ é€’å­—å…¸æ ¼å¼
        doc_dict = {
            "filename": doc_info.filename,
            "file_type": doc_info.file_type,
            "file_size": doc_info.file_size,
            "upload_time": doc_info.upload_time,
            "status": doc_info.status,
            "processed_at": doc_info.processed_at,
            "node_count": doc_info.node_count,
            "error_message": doc_info.error_message
        }
        background_tasks.add_task(process_document_background, file_id, doc_dict)
        
        return {
            "message": "æ–‡æ¡£å¤„ç†å·²å¼€å§‹",
            "file_id": file_id,
            "status": "processing"
        }
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_document_background(file_id: str, doc_info: Dict[str, Any]):
    """åå°å¤„ç†æ–‡æ¡£çš„ä»»åŠ¡"""
    try:
        logger.info(f"ğŸ“„ å¼€å§‹åå°å¤„ç†æ–‡æ¡£: {doc_info['filename']}")
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨find_file_by_idå‡½æ•°
        file_info = find_file_by_id(file_id)
        if file_info is None:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œfile_id: {file_id}")
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            docs_db = load_documents_db()
            if file_id in docs_db:
                doc_info = docs_db[file_id]
                doc_info.status = "failed"
                doc_info.error_message = "æ–‡ä»¶ä¸å­˜åœ¨"
                docs_db[file_id] = doc_info
                save_documents_db(docs_db)
            return
        
        file_path, original_filename = file_info
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        if doc_info["file_type"] == "pdf":
            await process_pdf_document(file_id, file_path, doc_info)
        elif doc_info["file_type"] in ["cad", "bim"]:
            await process_cad_bim_document(file_id, file_path, doc_info)
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {doc_info['file_type']}")
            # æ›´æ–°çŠ¶æ€ä¸ºä¸æ”¯æŒ
            docs_db = load_documents_db()
            if file_id in docs_db:
                doc_info_obj = docs_db[file_id]
                doc_info_obj.status = "failed"
                doc_info_obj.error_message = f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {doc_info['file_type']}"
                docs_db[file_id] = doc_info_obj
                save_documents_db(docs_db)
            
    except Exception as e:
        logger.error(f"âŒ åå°å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        docs_db = load_documents_db()
        if file_id in docs_db:
            doc_info = docs_db[file_id]
            doc_info.status = "failed"
            doc_info.error_message = str(e)
            docs_db[file_id] = doc_info
            save_documents_db(docs_db)

async def process_pdf_document(file_id: str, file_path: str, doc_info: Dict[str, Any]):
    """å¤„ç†PDFæ–‡æ¡£"""
    try:
        logger.info(f"ğŸ“– è§£æPDFæ–‡æ¡£: {doc_info['filename']}")
        
        # è§£æPDF
        from ..utils.pdf_parser import parse_pdf_file
        pdf_content = parse_pdf_file(file_path, enable_ocr=True)
        
        if not pdf_content or not pdf_content.text.strip():
            logger.warning(f"âš ï¸ PDFæ–‡æ¡£æ— æ–‡æœ¬å†…å®¹: {doc_info['filename']}")
            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆä½†æ— å†…å®¹
            docs_db = load_documents_db()
            doc_info = docs_db[file_id]
            doc_info.status = "completed"
            doc_info.node_count = 0
            doc_info.error_message = "æ–‡æ¡£æ— æ–‡æœ¬å†…å®¹"
            docs_db[file_id] = doc_info
            save_documents_db(docs_db)
            return
        
        logger.info(f"âœ… PDFè§£æå®Œæˆï¼Œæ–‡æœ¬é•¿åº¦: {len(pdf_content.text)} å­—ç¬¦")
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        logger.info(f"ğŸ•¸ï¸ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        from ..services.graphiti_service import get_graphiti_service
        
        # ä½¿ç”¨æ–°çš„GraphitiæœåŠ¡
        graphiti_service = get_graphiti_service()
        if graphiti_service is None:
            raise Exception("GraphitiæœåŠ¡æœªåˆå§‹åŒ–")
            
        result = await graphiti_service.build_knowledge_graph(
            text=pdf_content.text,
            document_id=file_id
        )
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        docs_db = load_documents_db()
        doc_info = docs_db[file_id]
        doc_info.status = "completed"
        doc_info.processed_at = datetime.now()
        doc_info.node_count = result.get("node_count", 0)
        
        if not result.get("success"):
            doc_info.error_message = result.get("error", "çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥")
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {result.get('error')}")
        else:
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸï¼ŒèŠ‚ç‚¹æ•°: {result.get('node_count', 0)}")
        
        docs_db[file_id] = doc_info
        save_documents_db(docs_db)
        
    except Exception as e:
        logger.error(f"âŒ PDFæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        docs_db = load_documents_db()
        if file_id in docs_db:
            doc_info = docs_db[file_id]
            doc_info.status = "failed"
            doc_info.error_message = str(e)
            docs_db[file_id] = doc_info
            save_documents_db(docs_db)

async def process_cad_bim_document(file_id: str, file_path: str, doc_info: Dict[str, Any]):
    """å¤„ç†CAD/BIMæ–‡æ¡£"""
    try:
        logger.info(f"ğŸ—ï¸ å¤„ç†CAD/BIMæ–‡æ¡£: {doc_info['filename']}")
        
        # TODO: å®ç°CAD/BIMæ–‡ä»¶è§£æ
        # ç›®å‰æš‚æ—¶æ ‡è®°ä¸ºä¸æ”¯æŒ
        
        docs_db = load_documents_db()
        doc_info = docs_db[file_id]
        doc_info.status = "completed"
        doc_info.processed_at = datetime.now()
        doc_info.node_count = 0
        doc_info.error_message = "CAD/BIMæ–‡ä»¶å¤„ç†åŠŸèƒ½å¾…å®ç°"
        docs_db[file_id] = doc_info
        save_documents_db(docs_db)
        
        logger.info(f"âš ï¸ CAD/BIMæ–‡æ¡£å¤„ç†å®Œæˆï¼ˆåŠŸèƒ½å¾…å®ç°ï¼‰")
        
    except Exception as e:
        logger.error(f"âŒ CAD/BIMæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        docs_db = load_documents_db()
        if file_id in docs_db:
            doc_info = docs_db[file_id]
            doc_info.status = "failed"
            doc_info.error_message = str(e)
            docs_db[file_id] = doc_info
            save_documents_db(docs_db)


@router.get("/list")
async def list_uploaded_files():
    """åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        # ä»æŒä¹…åŒ–å­˜å‚¨ä¸­è¯»å–æ–‡æ¡£ä¿¡æ¯
        documents = load_documents_db()
        files = []
        
        for file_id, doc_info in documents.items():
            # è·å–å½“å‰å¤„ç†çŠ¶æ€
            current_status = doc_info.status
            if file_id in processing_status:
                current_status = processing_status[file_id].status
            
            # è½¬æ¢ä¸ºAPIå“åº”æ ¼å¼
            file_data = {
                "file_id": file_id,
                "filename": doc_info.filename,
                "file_type": doc_info.file_type,
                "file_size": doc_info.file_size,
                "upload_time": doc_info.upload_time.timestamp() if isinstance(doc_info.upload_time, datetime) else doc_info.upload_time,
                "status": current_status,
                "node_count": doc_info.node_count,
                "processed_at": doc_info.processed_at.timestamp() if doc_info.processed_at and isinstance(doc_info.processed_at, datetime) else None,
                "error_message": doc_info.error_message
            }
            
            files.append(file_data)
        
        # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x["upload_time"], reverse=True)
        
        return {"files": files}
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """åˆ é™¤æ–‡ä»¶"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæŒä¹…åŒ–å­˜å‚¨ä¸­
        documents = load_documents_db()
        if file_id not in documents:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        deleted = False
        
        # åœ¨å„ä¸ªç›®å½•ä¸­æŸ¥æ‰¾å¹¶åˆ é™¤æ–‡ä»¶
        for subdir in ["pdf", "doc", "cad", "bim", "temp"]:
            dir_path = os.path.join(settings.UPLOAD_DIR, subdir)
            if os.path.exists(dir_path):
                for filename in os.listdir(dir_path):
                    if filename.startswith(file_id):
                        file_path = os.path.join(dir_path, filename)
                        os.remove(file_path)
                        deleted = True
                        logger.info(f"åˆ é™¤æ–‡ä»¶: {file_path}")
        
        # ä»æŒä¹…åŒ–å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£ä¿¡æ¯
        del documents[file_id]
        save_documents_db(documents)
        
        # åˆ é™¤å¤„ç†çŠ¶æ€
        if file_id in processing_status:
            del processing_status[file_id]
        
        return {"message": "æ–‡ä»¶åˆ é™¤æˆåŠŸ"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/stats")
async def get_document_stats():
    """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯ - çœŸå®æ•°æ®"""
    try:
        documents = load_documents_db()
        
        total_documents = len(documents)
        # Refined status checking for more accuracy
        processed_documents = len([doc for doc in documents.values() if doc.status == "completed"])
        processing_documents = len([doc for doc in documents.values() if doc.status in ["processing", "parsing", "building_graph", "processing_queued"]])
        # Consider various failure states
        failed_documents = len([doc for doc in documents.values() if "fail" in doc.status or "error" in doc.status])
        
        total_nodes = sum(doc.node_count for doc in documents.values() if isinstance(doc.node_count, int))
        
        file_types_counts = {}
        for doc in documents.values():
            file_types_counts[doc.file_type] = file_types_counts.get(doc.file_type, 0) + 1
        
        total_size_bytes_val = sum(doc.file_size for doc in documents.values() if isinstance(doc.file_size, int))
        
        # Placeholder for relations, actual count should come from graph_stats if available
        # This is a very rough estimate and might be misleading.
        # Consider linking this to graphiti_service.get_graph_stats() if a global stat is needed.
        estimated_total_relations = total_nodes * 2

        return {
            "total_documents": total_documents,
            "processed_documents": processed_documents,
            "processing_documents": processing_documents,
            "failed_documents": failed_documents,
            "total_extracted_nodes": total_nodes, # Renamed for clarity
            "estimated_total_relations": estimated_total_relations,
            "file_types": file_types_counts,
            "total_size_bytes": total_size_bytes_val,
            "total_size_mb": round(total_size_bytes_val / (1024 * 1024), 2) if total_size_bytes_val else 0.0
        }
    except Exception as e:
        logger.error("è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯å¤±è´¥", exc_info=True)
        # Return a more structured error response or raise HTTPException
        # For now, returning empty/zeroed stats as per original logic, but this could be an HTTPException too.
        # raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ã€‚")
        # Keeping original return type for now:
        return {
            "total_documents": 0, "processed_documents": 0, "processing_documents": 0,
            "failed_documents": 0, "total_extracted_nodes": 0, "estimated_total_relations": 0,
            "file_types": {}, "total_size_bytes": 0, "total_size_mb": 0.0,
            "error": "Failed to retrieve statistics." # Added error field
        }