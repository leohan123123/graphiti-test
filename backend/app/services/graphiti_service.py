"""
æ¡¥æ¢å·¥ç¨‹çŸ¥è¯†å›¾è°±æœåŠ¡
åŸºäº Graphiti æ¡†æ¶å®ç°çŸ¥è¯†å›¾è°±æ„å»ºå’Œç®¡ç†
"""
import os
import logging
import asyncio
import re
from typing import Optional, Dict, Any, List
from datetime import datetime
from pydantic import BaseModel
from pathlib import Path

from graphiti_core import Graphiti
from graphiti_core.nodes import EpisodeType
from ..core.config import get_settings
from ..utils.pdf_parser import PDFContent

logger = logging.getLogger(__name__)

class KnowledgeGraphResult(BaseModel):
    """çŸ¥è¯†å›¾è°±æ„å»ºç»“æœ"""
    success: bool
    entity_count: int
    relationship_count: int
    processing_time: float
    group_id: str
    error_message: Optional[str] = None

class SearchResult(BaseModel):
    """æœç´¢ç»“æœ"""
    entities: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    total_count: int

class GraphitiService:
    """Graphiti çŸ¥è¯†å›¾è°±æœåŠ¡"""
    
    def __init__(self):
        self.client: Optional[Graphiti] = None
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ– Graphiti å®¢æˆ·ç«¯"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Graphiti å®¢æˆ·ç«¯...")
            
            # æ–¹å¼1ï¼šä½¿ç”¨DeepSeek APIï¼ˆä¼˜å…ˆï¼‰
            try:
                from graphiti_core.llm_client.config import LLMConfig
                from app.services.deepseek_client import DeepSeekClient
                from app.services.deepseek_embedder import DeepSeekEmbedder
                
                # é…ç½® DeepSeek LLMï¼ˆä½¿ç”¨è‡ªå®šä¹‰å®¢æˆ·ç«¯ï¼‰
                llm_config = LLMConfig(
                    api_key="sk-0b26cde0319b451e984c38a0734353e7",
                    model="deepseek-chat",
                    base_url="https://api.deepseek.com/v1"
                )
                llm_client = DeepSeekClient(config=llm_config)
                
                # ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥å™¨ï¼Œé¿å…è°ƒç”¨OpenAI API
                embedder = DeepSeekEmbedder()
                
                # é…ç½®Graphitiï¼Œä½¿ç”¨DeepSeek LLMå’Œè‡ªå®šä¹‰åµŒå…¥å™¨
                self.client = Graphiti(
                    uri="bolt://localhost:7687",
                    user="neo4j", 
                    password="bridge123",
                    llm_client=llm_client,
                    embedder=embedder
                    # cross_encoder=None (é»˜è®¤ï¼Œé¿å…é¢å¤–APIè°ƒç”¨)
                )
                logger.info("âœ… Graphiti å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨DeepSeek APIï¼‰")
                return
            except Exception as e1:
                logger.warning(f"âš ï¸ DeepSeekåˆå§‹åŒ–å¤±è´¥: {e1}")
            
            # æ–¹å¼2ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„OpenAIé…ç½®ï¼ˆå¤‡ç”¨ï¼‰
            if os.environ.get("OPENAI_API_KEY"):
                try:
                    self.client = Graphiti(
                        uri="bolt://localhost:7687",
                        user="neo4j", 
                        password="bridge123"
                    )
                    logger.info("âœ… Graphiti å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨OpenAIé…ç½®ï¼‰")
                    return
                except Exception as e2:
                    logger.warning(f"âš ï¸ OpenAIåˆå§‹åŒ–å¤±è´¥: {e2}")
            
            # æ–¹å¼3ï¼šæœ€ç®€å•çš„åˆå§‹åŒ–ï¼ˆæœ€åå¤‡ç”¨ï¼‰
            try:
                self.client = Graphiti(
                    uri="bolt://localhost:7687",
                    user="neo4j", 
                    password="bridge123"
                )
                logger.info("âœ… Graphiti å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰")
                return
            except Exception as e3:
                logger.warning(f"âš ï¸ é»˜è®¤åˆå§‹åŒ–å¤±è´¥: {e3}")
            
            raise Exception("æ‰€æœ‰åˆå§‹åŒ–æ–¹å¼éƒ½å¤±è´¥äº†")
            
        except Exception as e:
            logger.error(f"âŒ Graphiti å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None


    def is_available(self) -> bool:
        """æ£€æŸ¥ Graphiti æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    async def build_knowledge_graph(self, text: str, document_id: str) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        if not self.is_available():
            logger.error("âŒ Graphiti å®¢æˆ·ç«¯ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "Graphiti å®¢æˆ·ç«¯ä¸å¯ç”¨",
                "node_count": 0,
                "edge_count": 0
            }
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹ä¸ºæ–‡æ¡£ {document_id} æ„å»ºçŸ¥è¯†å›¾è°±...")
            logger.info(f"ğŸ“„ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            
            # æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†å›¾è°±
            episode_name = f"document_{document_id}"
            
            # ä½¿ç”¨ Graphiti æ·»åŠ æ–‡æ¡£
            logger.info("ğŸ“ æ­£åœ¨æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†å›¾è°±...")
            
            # è°ƒç”¨ Graphiti çš„ add_episode æ–¹æ³•
            episode_result = await self.client.add_episode(
                name=episode_name,
                episode_body=text,
                source_description="æ¡¥æ¢å·¥ç¨‹æŠ€æœ¯æ–‡æ¡£",
                reference_time=datetime.now(),
                source=EpisodeType.text  # ä½¿ç”¨æ–‡æœ¬ç±»å‹
                
            )
            
            logger.info(f"âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ: {episode_result}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = await self.get_graph_stats()
            
            logger.info(f"ğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡: èŠ‚ç‚¹æ•°={stats.get('node_count', 0)}, è¾¹æ•°={stats.get('edge_count', 0)}")
            
            return {
                "success": True,
                "episode_name": episode_name,
                "node_count": stats.get('node_count', 0),
                "edge_count": stats.get('edge_count', 0),
                "message": "çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ"
            }
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "node_count": 0,
                "edge_count": 0
            }
    
    async def search_knowledge(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """æœç´¢çŸ¥è¯†å›¾è°±"""
        if not self.is_available():
            return {
                "success": False,
                "error": "Graphiti å®¢æˆ·ç«¯ä¸å¯ç”¨",
                "entities": [],
                "relationships": [],
                "total_count": 0
            }
        
        try:
            logger.info(f"ğŸ” æœç´¢çŸ¥è¯†å›¾è°±: {query}")
            
            # ä½¿ç”¨ Graphiti çš„æœç´¢åŠŸèƒ½
            search_results = await self.client.search(
                query=query,
                num_results=limit
            )
            
            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
            
            # åˆ†ç¦»å®ä½“å’Œå…³ç³»ï¼ˆå¦‚æœæœç´¢ç»“æœåŒ…å«è¿™äº›ä¿¡æ¯ï¼‰
            entities = []
            relationships = []
            
            for result in search_results:
                # æ ¹æ®ç»“æœç±»å‹åˆ†ç±»
                if hasattr(result, 'uuid') and hasattr(result, 'name'):
                    # è¿™æ˜¯ä¸€ä¸ªå®ä½“
                    entities.append({
                        "uuid": getattr(result, 'uuid', ''),
                        "name": getattr(result, 'name', ''),
                        "summary": getattr(result, 'summary', ''),
                        "type": getattr(result, 'type', 'entity')
                    })
                else:
                    # å…¶ä»–ç±»å‹çš„ç»“æœ
                    entities.append({
                        "uuid": str(id(result)),
                        "name": str(result)[:100] if len(str(result)) > 100 else str(result),
                        "summary": str(result),
                        "type": "search_result"
                    })
            
            return {
                "success": True,
                "entities": entities,
                "relationships": relationships,
                "total_count": len(search_results)
            }
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æœç´¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "entities": [],
                "relationships": [],
                "total_count": 0
            }
    
    async def get_graph_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_available():
            return {
                "node_count": 0,
                "edge_count": 0,
                "episode_count": 0,
                "status": "ä¸å¯ç”¨"
            }
        
        try:
            # ä»Neo4jæ•°æ®åº“ä¸­è·å–çœŸå®çš„ç»Ÿè®¡ä¿¡æ¯
            from neo4j import GraphDatabase
            
            # ä½¿ç”¨ç›¸åŒçš„è¿æ¥é…ç½®
            driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "bridge123")
            )
            
            with driver.session() as session:
                # è·å–èŠ‚ç‚¹æ•°é‡
                node_result = session.run("MATCH (n) RETURN count(n) as node_count")
                node_count = node_result.single()["node_count"]
                
                # è·å–å…³ç³»æ•°é‡
                edge_result = session.run("MATCH ()-[r]->() RETURN count(r) as edge_count")
                edge_count = edge_result.single()["edge_count"]
                
                # è·å–EpisodeèŠ‚ç‚¹æ•°é‡
                episode_result = session.run("MATCH (n:Episodic) RETURN count(n) as episode_count")
                episode_count = episode_result.single()["episode_count"]
            
            driver.close()
            
            logger.info(f"ğŸ“Š è·å–åˆ°çœŸå®ç»Ÿè®¡æ•°æ®: èŠ‚ç‚¹={node_count}, å…³ç³»={edge_count}, Episodes={episode_count}")
            
            return {
                "node_count": node_count,
                "edge_count": edge_count,
                "episode_count": episode_count,
                "status": "æ­£å¸¸"
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "node_count": 0,
                "edge_count": 0,
                "episode_count": 0,
                "status": f"é”™è¯¯: {e}"
            }
    def get_health_status(self) -> Dict[str, Any]:
        """è·å–å¥åº·çŠ¶æ€"""
        return {
            "status": "healthy" if self.is_available() else "unhealthy",
            "client_available": self.is_available(),
            "neo4j_connected": self.is_available(),  # ç®€åŒ–æ£€æŸ¥
            "message": "Graphiti æœåŠ¡æ­£å¸¸" if self.is_available() else "Graphiti æœåŠ¡ä¸å¯ç”¨"
        }

# å…¨å±€å®ä¾‹
graphiti_service = GraphitiService()

async def build_knowledge_graph_from_pdf(
    pdf_content: PDFContent,
    document_name: str,
    group_id: Optional[str] = None
) -> KnowledgeGraphResult:
    """
    ä» PDF å†…å®¹æ„å»ºçŸ¥è¯†å›¾è°±
    
    Args:
        pdf_content: PDF è§£æå†…å®¹
        document_name: æ–‡æ¡£åç§°
        group_id: åˆ†ç»„IDï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¡¥æ¢å·¥ç¨‹åˆ†ç»„
        
    Returns:
        KnowledgeGraphResult: æ„å»ºç»“æœ
    """
    if not graphiti_service.is_available():
        return KnowledgeGraphResult(
            success=False,
            entity_count=0,
            relationship_count=0,
            processing_time=0.0,
            group_id="",
            error_message="Graphiti å®¢æˆ·ç«¯ä¸å¯ç”¨"
        )
    
    start_time = datetime.now()
    # ç›´æ¥ä½¿ç”¨é»˜è®¤å€¼ï¼Œé¿å…Settingså†²çª
    target_group_id = group_id or "bridge_engineering"
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°± - æ–‡æ¡£: {document_name}")
        logger.info(f"ğŸ“Š æ–‡æœ¬é•¿åº¦: {len(pdf_content.text)} å­—ç¬¦")
        
        # åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬ï¼Œé¿å…è¶…è¿‡APIé™åˆ¶
        text_chunks = _split_text(pdf_content.text, max_chunk_size=3000)
        total_entities = 0
        total_relationships = 0
        
        for i, chunk in enumerate(text_chunks):
            logger.info(f"ğŸ“ å¤„ç†æ–‡æœ¬å— {i+1}/{len(text_chunks)}")
            
            # æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_delay = 5  # ç§’
            
            for attempt in range(max_retries):
                try:
                    # æ„å»ºçŸ¥è¯†å›¾è°±
                    episode_result = await graphiti_service.build_knowledge_graph(
                        chunk,
                        f"{document_name}_chunk_{i+1}"
                    )
                    
                    logger.info(f"âœ… æ–‡æœ¬å— {i+1} å¤„ç†æˆåŠŸ")
                    break
                    
                except Exception as e:
                    error_msg = str(e)
                    if "Rate limit exceeded" in error_msg:
                        if attempt < max_retries - 1:
                            logger.warning(f"â³ é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {retry_delay} ç§’åé‡è¯• (å°è¯• {attempt+1}/{max_retries})")
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 2  # æŒ‡æ•°é€€é¿
                            continue
                        else:
                            logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ–‡æœ¬å— {i+1}")
                    else:
                        logger.error(f"âŒ æ–‡æœ¬å— {i+1} å¤„ç†å¤±è´¥: {error_msg}")
                        break
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        try:
            stats = await graphiti_service.get_graph_stats()
            total_entities = stats.get("node_count", 0)
            total_relationships = stats.get("edge_count", 0)
        except Exception as e:
            logger.warning(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        logger.info(f"ğŸ“Š å®ä½“æ•°é‡: {total_entities}, å…³ç³»æ•°é‡: {total_relationships}")
        logger.info(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        
        return KnowledgeGraphResult(
            success=True,
            entity_count=total_entities,
            relationship_count=total_relationships,
            processing_time=processing_time,
            group_id=target_group_id,
            error_message=None
        )
        
    except Exception as e:
        processing_time = (datetime.now() - start_time).total_seconds()
        error_message = f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_message}")
        
        return KnowledgeGraphResult(
            success=False,
            entity_count=0,
            relationship_count=0,
            processing_time=processing_time,
            group_id=target_group_id,
            error_message=error_message
        )

def _prepare_episode_content(pdf_content: PDFContent, document_name: str) -> str:
    """
    å‡†å¤‡ç”¨äº Graphiti çš„å†…å®¹
    
    Args:
        pdf_content: PDF å†…å®¹
        document_name: æ–‡æ¡£åç§°
        
    Returns:
        str: æ ¼å¼åŒ–çš„å†…å®¹
    """
    content_parts = []
    
    # æ·»åŠ æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
    content_parts.append(f"æ–‡æ¡£åç§°: {document_name}")
    content_parts.append(f"é¡µæ•°: {pdf_content.page_count}")
    content_parts.append(f"æ–‡æ¡£ç±»å‹: æ¡¥æ¢å·¥ç¨‹æŠ€æœ¯æ–‡æ¡£")
    
    # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
    if pdf_content.metadata.get("title"):
        content_parts.append(f"æ–‡æ¡£æ ‡é¢˜: {pdf_content.metadata['title']}")
    if pdf_content.metadata.get("author"):
        content_parts.append(f"ä½œè€…: {pdf_content.metadata['author']}")
    if pdf_content.metadata.get("subject"):
        content_parts.append(f"ä¸»é¢˜: {pdf_content.metadata['subject']}")
    
    # æ·»åŠ ä¸»è¦æ–‡æœ¬å†…å®¹ - è¿™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†
    content_parts.append("\n=== æ¡¥æ¢å·¥ç¨‹æŠ€æœ¯å†…å®¹ ===")
    
    # æ¸…ç†å¹¶ä¼˜åŒ–æ–‡æœ¬å†…å®¹ç”¨äºçŸ¥è¯†å›¾è°±æå–
    clean_text = _clean_text_for_kg(pdf_content.text)
    content_parts.append(clean_text)
    
    # æ·»åŠ  OCR è¯†åˆ«çš„å›¾åƒæ–‡æœ¬
    ocr_texts = []
    for image in pdf_content.images:
        if image.get("ocr_text"):
            ocr_texts.append(f"å›¾åƒ {image['name']}: {image['ocr_text']}")
    
    if ocr_texts:
        content_parts.append("\n=== å›¾åƒè¯†åˆ«çš„æŠ€æœ¯å†…å®¹ ===")
        content_parts.extend(ocr_texts)
    
    # æ·»åŠ è¡¨æ ¼ä¿¡æ¯
    if pdf_content.tables:
        content_parts.append("\n=== è¡¨æ ¼æ•°æ® ===")
        for i, table in enumerate(pdf_content.tables):
            content_parts.append(f"è¡¨æ ¼ {i+1}: {table.get('summary', 'æŠ€æœ¯æ•°æ®è¡¨')}")
    
    final_content = "\n".join(content_parts)
    
    # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…å¤ªé•¿çš„æ–‡æœ¬å½±å“LLMå¤„ç†
    if len(final_content) > 50000:  # 50Kå­—ç¬¦é™åˆ¶
        logger.warning(f"æ–‡æ¡£å†…å®¹è¿‡é•¿({len(final_content)}å­—ç¬¦)ï¼Œå°†æˆªæ–­åˆ°50Kå­—ç¬¦")
        final_content = final_content[:50000] + "\n...[å†…å®¹å·²æˆªæ–­]"
    
    return final_content

def _clean_text_for_kg(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ç”¨äºçŸ¥è¯†å›¾è°±æå–
    """
    import re
    
    # ç§»é™¤è¿‡å¤šçš„ç©ºè¡Œ
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    # ç§»é™¤é¡µçœ‰é¡µè„šæ ‡è®°
    text = re.sub(r'--- ç¬¬ \d+ é¡µ ---', '', text)
    
    # ç§»é™¤è¿‡å¤šçš„ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    
    # ä¿ç•™æœ‰æ„ä¹‰çš„æ¢è¡Œï¼Œä½†ç§»é™¤å­¤ç«‹çš„çŸ­è¡Œ
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        if len(line) > 3:  # ä¿ç•™é•¿åº¦å¤§äº3çš„è¡Œ
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def _split_text(text: str, max_chunk_size: int = 3000) -> List[str]:
    """
    å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—
    
    Args:
        text: éœ€è¦åˆ†å‰²çš„æ–‡æœ¬
        max_chunk_size: æ¯å—çš„æœ€å¤§å­—ç¬¦æ•°
        
    Returns:
        List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    """
    if len(text) <= max_chunk_size:
        return [text]
    
    chunks = []
    current_pos = 0
    
    while current_pos < len(text):
        # è®¡ç®—å½“å‰å—çš„ç»“æŸä½ç½®
        end_pos = min(current_pos + max_chunk_size, len(text))
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·æˆ–æ¢è¡Œç¬¦å¤„åˆ†å‰²
        if end_pos < len(text):
            # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„å¥å·æˆ–æ¢è¡Œç¬¦
            for i in range(end_pos, current_pos, -1):
                if text[i] in '.ã€‚\n':
                    end_pos = i + 1
                    break
        
        chunk = text[current_pos:end_pos].strip()
        if chunk:
            chunks.append(chunk)
        
        current_pos = end_pos
    
    return chunks

async def search_entities(
    query: str, 
    group_id: Optional[str] = None,
    limit: int = 50
) -> SearchResult:
    """
    æœç´¢å®ä½“
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        group_id: åˆ†ç»„ID
        limit: ç»“æœé™åˆ¶æ•°é‡
        
    Returns:
        SearchResult: æœç´¢ç»“æœ
    """
    if not graphiti_service.is_available():
        return SearchResult(entities=[], relationships=[], total_count=0)
    
    try:
        actual_group_id = group_id or get_settings().GRAPHITI_GROUP_ID
        
        # ä½¿ç”¨ Graphiti æœç´¢
        search_results = await graphiti_service.search_knowledge(
            query=query,
            limit=limit
        )
        
        entities = search_results.get("entities", [])
        
        return SearchResult(
            entities=entities,
            relationships=search_results.get("relationships", []),
            total_count=search_results.get("total_count", 0)
        )
        
    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {str(e)}")
        return SearchResult(entities=[], relationships=[], total_count=0)

async def get_entity_relationships(
    entity_uuid: str, 
    group_id: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    è·å–å®ä½“çš„å…³ç³»
    
    Args:
        entity_uuid: å®ä½“ UUID
        group_id: åˆ†ç»„ID
        
    Returns:
        List[Dict[str, Any]]: å…³ç³»åˆ—è¡¨
    """
    # TODO: å®ç°å®ä½“å…³ç³»æŸ¥è¯¢
    # éœ€è¦ç›´æ¥æŸ¥è¯¢ Neo4j æ•°æ®åº“
    return []

async def export_knowledge_corpus(
    group_id: Optional[str] = None,
    format_type: str = "jsonl"
) -> str:
    """
    å¯¼å‡ºçŸ¥è¯†å›¾è°±ä½œä¸ºè®­ç»ƒè¯­æ–™
    
    Args:
        group_id: åˆ†ç»„ID
        format_type: å¯¼å‡ºæ ¼å¼ (jsonl, txt, csv)
        
    Returns:
        str: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
    """
    actual_group_id = group_id or get_settings().GRAPHITI_GROUP_ID
    
    try:
        # è·å–æ‰€æœ‰å®ä½“å’Œå…³ç³»
        search_result = await search_entities(query="", group_id=actual_group_id, limit=10000)
        
        # æ ¹æ®æ ¼å¼å¯¼å‡º
        if format_type == "jsonl":
            return await _export_jsonl(search_result, actual_group_id)
        elif format_type == "txt":
            return await _export_txt(search_result, actual_group_id)
        elif format_type == "csv":
            return await _export_csv(search_result, actual_group_id)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºçŸ¥è¯†è¯­æ–™å¤±è´¥: {str(e)}")
        raise

async def _export_jsonl(search_result: SearchResult, group_id: str) -> str:
    """å¯¼å‡ºä¸º JSONL æ ¼å¼"""
    import json
    
    output_file = f"exports/knowledge_corpus_{group_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    
    # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for entity in search_result.entities:
            # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
            corpus_item = {
                "text": entity.get("summary", ""),
                "entity": entity.get("name", ""),
                "domain": "bridge_engineering",
                "source": "knowledge_graph"
            }
            f.write(json.dumps(corpus_item, ensure_ascii=False) + '\n')
    
    return output_file

async def _export_txt(search_result: SearchResult, group_id: str) -> str:
    """å¯¼å‡ºä¸ºæ–‡æœ¬æ ¼å¼"""
    output_file = f"exports/knowledge_corpus_{group_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for entity in search_result.entities:
            f.write(f"å®ä½“: {entity.get('name', '')}\n")
            f.write(f"æè¿°: {entity.get('summary', '')}\n")
            f.write("=" * 50 + "\n")
    
    return output_file

async def _export_csv(search_result: SearchResult, group_id: str) -> str:
    """å¯¼å‡ºä¸º CSV æ ¼å¼"""
    import csv
    
    output_file = f"exports/knowledge_corpus_{group_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['entity_name', 'summary', 'domain', 'source'])
        
        for entity in search_result.entities:
            writer.writerow([
                entity.get('name', ''),
                entity.get('summary', ''),
                'bridge_engineering',
                'knowledge_graph'
            ])
    
    return output_file

# å…¨å±€æœåŠ¡å®ä¾‹
graphiti_service: Optional[GraphitiService] = None

def get_graphiti_service() -> GraphitiService:
    """è·å– Graphiti æœåŠ¡å®ä¾‹"""
    global graphiti_service
    
    if graphiti_service is None:
        logger.info("ğŸ”§ åˆå§‹åŒ– Graphiti æœåŠ¡...")
        try:
            graphiti_service = GraphitiService()
            logger.info("âœ… Graphiti æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Graphiti æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆ›å»ºä¸€ä¸ªä¸å¯ç”¨çš„æœåŠ¡å®ä¾‹
            graphiti_service = GraphitiService.__new__(GraphitiService)
            graphiti_service.client = None
    
    return graphiti_service

def reset_graphiti_service():
    """é‡ç½® Graphiti æœåŠ¡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global graphiti_service
    graphiti_service = None 